import os 
import os
from typing import Annotated, List, Tuple, Union
from langchain.tools import BaseTool, StructuredTool, Tool
from langchain_experimental.tools import PythonREPLTool
from langchain_core.tools import tool
import random
from bs4 import BeautifulSoup
import requests
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import json
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain import PromptTemplate
from langchain.agents import initialize_agent, Tool 
from langchain.agents import AgentType
from langchain.chat_models import ChatOpenAI
from langchain.prompts import MessagesPlaceholder
from langchain.memory import ConversationSummaryBufferMemory 
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
from langchain.tools import BaseTool
from pydantic import BaseModel, Field
from typing import Type
from bs4 import BeautifulSoup
import requests
import json
from langchain.schema import SystemMessage
from typing import Any, Callable, List, Optional, TypedDict, Union
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import Runnable
from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "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"

def scrap_web(url: str):
    """Returns the contents from a website, input the web site link"""
    print("Scrapping from Website...")
    print(url)

   # options = Options()
    #options.add_argument('--headless') 
    # options.addArguments("--no-sandbox")
   # options.add_argument('--remote-debugging-pipe')

 # Adds the headless option
   ## options.addArguments("--disable-dev-shm-usage")


# Initialize the Chrome WebDriver with the specified options
    driver = webdriver.Chrome()
    #driver = webdriver.Chrome(options=options)

    driver.get(url)
    driver.implicitly_wait(10)
    page_source = driver.page_source
    driver.quit()
    print(type(page_source)) 
   # soup = BeautifulSoup(page_source, 'html.parser')
   # text = soup.get_text()

    #print("CONTENT: ", text)
    print("Scrapping done")
    #print(text[:100])

    #return text[:10000]
    #print(text)
#    text1 = [line.strip() for line in text]

#    if len(text1)>10000:
#         print("return")
#         output= summary(text)
#         return output


#    else:
    return page_source


def summary(content):
    llm=ChatOpenAI(temperature=0,model="gpt-3.5-turbo-16k-0613")

    text_splitter=RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n"], chunk_size=1000,
        chunk_overlap=0
    )
    doc= text_splitter.create_documents([content])
    map_prompt="""
    Write a summary of the folloewing text:
    "{text}"
    SUMMARY:
    """
    map_prompt_template= PromptTemplate(input_variables=["text"],
                                         template=map_prompt)
    summary_chain= load_summarize_chain(llm, 
                                        chain_type="map_reduce",
                                        map_prompt=map_prompt_template,
                                        combine_prompt=map_prompt_template,
                                        verbose=False)
    
    out=summary_chain.run(objective="Summarize without losing any key points and values ,remove unwanted terms",input_documents=doc)

    print("SUMMARY done")
    return out

print(scrap_web("https://psgitech.ac.in/"))
